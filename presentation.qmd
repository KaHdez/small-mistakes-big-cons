---
title: "Small Errors, Big Consequences: Lessons from Data Mismanagement in Business"
author: "Karla Hernández - 400916125 - hernandez.karla@stud.hs-fresenius.de"
format:
  revealjs:
    embed-resources: true
  pdf:
    papersize: A4
editor: visual
---

## Scan me

::: {#}
![](qr-code.png){width="60%"}
:::

## Why Smalls Errors Matter

-   Data drives decisions in production, hiring, marketing & forecasting
-   Most business data failures are not dramatic bugs
-   They come from tiny, unnoticed mistakes
-   If the data is wrong, the decision is wrong
-   RStudio does not warn when results are logically wrong

## What Counts as a "Small Error"?

::: {#tbl}
| Error                 | What it Means                             |
|-----------------------|-------------------------------------------|
| “Mexico” vs “México”  | R treats them as two different categories |
| Number stored as text | Math doesn't work the way we expect       |
| Missing values        | Averages and totals become wrong          |
| Duplicate rows        | Customers or employees counted twice      |
| Wrong join            | Rows multiply or disappear silently       |
:::

## Tiny Errors, Big Consequences

<div>

| Area | Tiny Error | Big Result |
|------------------------|------------------------|------------------------|
| Retail | Missing value in sales | Too little production that leads to lost on sales |
| HR | Employee duplicated during merge | Over-hired people that leads to unnecessary cost |
| Marketing | Duplicated customers | Overspent on ads |

</div>

## Bad Practice vs Good Practice

*Scenario:*

What is the average purchase amount of customers under 30, and how does it differ by location?

-   Click to download dataset: <a href="shopping_behavior.csv" download>CSV shopping_behavior dataset</a>

## Bad Practice Example

``` r
# BAD import: let R guess names and types
data_bad <- read.csv("shopping_behavior.csv")

# BAD filter (forgetting to handle NA)
young_bad <- data_bad[data_bad$Age < 30, ]

# BAD join using merge() with the same location_info
location_info <- data.frame(
  Location = c("California", "New York", "Texas", "Florida"),
  Region   = c("West", "East", "South", "South"),
  stringsAsFactors = FALSE
)

merged_bad <- merge(young_bad, location_info)

# BAD summary: mean without na.rm and no cleaning of Location variants
result_bad <- aggregate(Purchase.Amount..USD. ~ Location, data = merged_bad, FUN = mean)

# Rename columns to match good version later
names(result_bad) <- c("Location_clean", "avg_spend_bad")
result_bad$avg_spend_bad <- round(result_bad$avg_spend_bad, 2)

result_bad
print(result_bad, n = Inf)
View(result_bad)
```

## Outcome

```{r}
# BAD import: let R guess names and types
data_bad <- read.csv("shopping_behavior.csv")

# BAD filter (forgetting to handle NA, using base [] correctly this time but no cleaning)
young_bad <- data_bad[data_bad$Age < 30, ]

# BAD join using merge() with the same location_info
location_info <- data.frame(
  Location = c("California", "New York", "Texas", "Florida"),
  Region   = c("West", "East", "South", "South"),
  stringsAsFactors = FALSE
)

merged_bad <- merge(young_bad, location_info)

# BAD summary: mean without na.rm and no cleaning of Location variants
result_bad <- aggregate(Purchase.Amount..USD. ~ Location, data = merged_bad, FUN = mean)

# Rename columns to match good version later
names(result_bad) <- c("Location_clean", "avg_spend_bad")
result_bad$avg_spend_bad <- round(result_bad$avg_spend_bad, 2)

# Bad practice result
library(knitr)
library(kableExtra)

result_bad |>
  kbl(caption = "Bad practice result") |>
  scroll_box(height = "600px")
```

## Plot

``` r
# BAD plot result
barplot(
  result_bad$avg_spend_bad,
  names.arg = result_bad$Location,
  main = "Average Spend (BAD PRACTICE)",
  xlab = "Location",
  ylab = "Average Spend (USD)"
)
```

**Plot Outcome**
``` {r}
# BAD plot result
barplot(
  result_bad$avg_spend_bad,
  names.arg = result_bad$Location,
  main = "Average Spend (BAD PRACTICE)",
  xlab = "Location",
  ylab = "Average Spend (USD)"
)
```

## Good Practice Example

``` r
# Example of a good practice
# 1) Import packages
library(dplyr)
library(lubridate)
library(ggplot2)

# 2) Read data with good practices
data_good <- read.csv(
  "shopping_behavior.csv",
  stringsAsFactors = FALSE,
  check.names = FALSE   # keeps "Purchase Amount (USD)" as-is
)

# 3) Parse dates correctly
data_good <- data_good %>%
  mutate(
    Date_good = dmy(Date)
  )

# 4) Remove exact duplicate rows (we know we injected some)
data_nodup <- data_good %>%
  distinct(.keep_all = TRUE)

# 5) Keep only customers under 30
young_good <- data_nodup %>%
  filter(Age < 30)

# 6) Location lookup table (regions are just for context)
location_info <- data.frame(
  Location = c("California", "New York", "Texas", "Florida"),
  Region   = c("West", "East", "South", "South"),
  stringsAsFactors = FALSE
)

# 7) Safe join: keep all young customers, add Region if Location matches
merged_good <- young_good %>%
  left_join(location_info, by = "Location")

# 8) Clean up Location variants into one standard name
final_clean <- merged_good %>%
  mutate(
    Location_clean = case_when(
      Location %in% c("California", "california", "Californi", "CA") ~ "California",
      Location %in% c("New York", "NewYork", "N. York")              ~ "New York",
      TRUE ~ Location
    )
  ) %>%
  group_by(Location_clean) %>%
  summarise(
    avg_spend_young = mean(`Purchase Amount (USD)`, na.rm = TRUE),
    n_customers     = n(),
    .groups = "drop"
  ) %>%
  arrange(Location_clean)

# 9) Round numbers to make the table nice for printing
final_clean$avg_spend_young <- round(final_clean$avg_spend_young, 2)

# 10) Print final result nicely
final_clean

print(final_clean, n = Inf)
View(final_clean)
```

## Outcome

```{r}
# Example of a good practice
# 1) Import packages
library(dplyr)
library(lubridate)
library(ggplot2)

# 2) Read data with good practices
data_good <- read.csv(
  "shopping_behavior.csv",
  stringsAsFactors = FALSE,
  check.names = FALSE   # keeps "Purchase Amount (USD)" as-is
)

# 3) Parse dates correctly
data_good <- data_good %>%
  mutate(
    Date_good = dmy(Date)
  )

# 4) Remove exact duplicate rows (we know we injected some)
data_nodup <- data_good %>%
  distinct(.keep_all = TRUE)

# 5) Keep only customers under 30
young_good <- data_nodup %>%
  filter(Age < 30)

# 6) Location lookup table (regions are just for context)
location_info <- data.frame(
  Location = c("California", "New York", "Texas", "Florida"),
  Region   = c("West", "East", "South", "South"),
  stringsAsFactors = FALSE
)

# 7) Safe join: keep all young customers, add Region if Location matches
merged_good <- young_good %>%
  left_join(location_info, by = "Location")

# 8) Clean up Location variants into one standard name
final_clean <- merged_good %>%
  mutate(
    Location_clean = case_when(
      Location %in% c("California", "california", "Californi", "CA") ~ "California",
      Location %in% c("New York", "NewYork", "N. York")              ~ "New York",
      TRUE ~ Location
    )
  ) %>%
  group_by(Location_clean) %>%
  summarise(
    avg_spend_young = mean(`Purchase Amount (USD)`, na.rm = TRUE),
    n_customers     = n(),
    .groups = "drop"
  ) %>%
  arrange(Location_clean)

# 9) Round numbers to make the table nice for printing
final_clean$avg_spend_young <- round(final_clean$avg_spend_young, 2)

# 10) Print final result nicely
final_clean 
```

## Plot

``` r
# 11) Plot final results
ggplot(plot_data, aes(x = Location_clean, y = avg_spend_young, fill = Location_clean)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  coord_flip() +
  labs(
    title = "Average Spend of Customers Under 30 by Location",
    x = "Location",
    y = "Average Spend (USD)"
  ) +
  scale_fill_viridis_d() +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 20, hjust = 0.5),
    axis.text.y = element_text(size = 8),
    axis.text.x = element_text(size = 8)
  )
```

## Plot Outcome

::: {#}
![](good-practice-plot-outcome.png){width="60%"}
:::


## Lessons from Data Mismanagement in Business

1.  A *tiny mistake* in data can cause a **huge mistake** in a business decision.
2.  RStudio won't warn you when the result is *wrong*, only when the **code** is.
3.  Never *assume* the data is clean, always **check it**.
4.  Good analysis starts **before** modeling with data validation.
5.  *Being explicit* in your code **prevents disasters**.

## 

6.  Documentation and reproducible scripts **protect** you from *mistakes*.
7.  *Joins* are one of the most dangerous operations, **check them twice**.
8.  **Data types** matter more than most people realize.
9.  *Peer* review is **essential**.
10. ***Great analysts aren't those who make no mistakes, they are the ones who catch mistakes early.***